{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from typing import Dict, Tuple, TypeVar\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "from gymnasium import spaces\n",
    "from plotly.subplots import make_subplots\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm\n",
    "from dataclasses import dataclass\n",
    "\n",
    "sys.path.append(\"../../..\")\n",
    "from lasertag import LasertagAdversarial  # noqa: E402\n",
    "from syllabus.core import (  # noqa: E402\n",
    "    DualCurriculumWrapper,\n",
    "    TaskWrapper,\n",
    "    make_multiprocessing_curriculum,\n",
    ")\n",
    "\n",
    "# noqa: E402\n",
    "from syllabus.curricula import (  # noqa: E402\n",
    "    CentralizedPrioritizedLevelReplay,\n",
    "    DomainRandomization,\n",
    "    FictitiousSelfPlay,\n",
    "    PrioritizedFictitiousSelfPlay,\n",
    "    SelfPlay,\n",
    ")\n",
    "from syllabus.task_space import TaskSpace  # noqa: E402\n",
    "\n",
    "ActionType = TypeVar(\"ActionType\")\n",
    "AgentID = TypeVar(\"AgentID\")\n",
    "AgentType = TypeVar(\"AgentType\")\n",
    "EnvTask = TypeVar(\"EnvTask\")\n",
    "AgentTask = TypeVar(\"AgentTask\")\n",
    "ObsType = TypeVar(\"ObsType\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Args:\n",
    "    track: bool = False\n",
    "    total_updates: int = 4000\n",
    "    rollout_length: int = 256\n",
    "    batch_size: int = 32\n",
    "    ent_coef: float = 0.0\n",
    "    vf_coef: float = 0.5\n",
    "    clip_coef: float = 0.2\n",
    "    learning_rate: float = 1e-4\n",
    "    epsilon: float = 1e-5\n",
    "    gamma: float = 0.995\n",
    "    gae_lambda: float = 0.95\n",
    "    epochs: int = 5\n",
    "    agent_curriculum: str = \"SP\"\n",
    "    env_curriculum: str = \"DR\"\n",
    "    agent_update_frequency: int = 8000\n",
    "    max_agents: int = 10\n",
    "    save_agent_checkpoints: bool = False\n",
    "    checkpoint_frequency: int = 4000\n",
    "    n_env_tasks: int = 4000\n",
    "    seed: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(x, device):\n",
    "    \"\"\"Converts PZ style returns to batch of torch arrays.\"\"\"\n",
    "    # convert to list of np arrays\n",
    "    x = np.stack([x[a] for a in x], axis=0)\n",
    "    # convert to torch\n",
    "    x = torch.tensor(x).to(device)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def unbatchify(x, possible_agents: np.ndarray):\n",
    "    \"\"\"Converts np array to PZ style arguments.\"\"\"\n",
    "    x = x.cpu().numpy()\n",
    "    x = {agent: x[idx] for idx, agent in enumerate(possible_agents)}\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "class LasertagParallelWrapper(TaskWrapper):\n",
    "    \"\"\"\n",
    "    Wrapper ensuring compatibility with the PettingZoo Parallel API.\n",
    "\n",
    "    Lasertag Environment:\n",
    "        * Action shape:  `n_agents` * `Discrete(5)`\n",
    "        * Observation shape: Dict('image': Box(0, 255, (`n_agents`, 3, 5, 5), uint8))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_agents, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.n_agents = n_agents\n",
    "        self.task = None\n",
    "        self.episode_return = 0\n",
    "        self.possible_agents = [f\"agent_{i}\" for i in range(self.n_agents)]\n",
    "        self.n_steps = 0\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        \"\"\"\n",
    "        Delegate attribute lookup to the wrapped environment if the attribute\n",
    "        is not found in the LasertagParallelWrapper instance.\n",
    "        \"\"\"\n",
    "        return getattr(self.env, name)\n",
    "\n",
    "    def _np_array_to_pz_dict(self, array: np.ndarray) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Returns a dictionary containing individual observations for each agent.\n",
    "        Assumes that the batch dimension represents individual agents.\n",
    "        \"\"\"\n",
    "        out = {}\n",
    "        for idx, value in enumerate(array):\n",
    "            out[self.possible_agents[idx]] = value\n",
    "        return out\n",
    "\n",
    "    def _singleton_to_pz_dict(self, value: bool) -> Dict[str, bool]:\n",
    "        \"\"\"\n",
    "        Broadcasts the `done` and `trunc` flags to dictionaries keyed by agent id.\n",
    "        \"\"\"\n",
    "        return {str(agent_index): value for agent_index in range(self.n_agents)}\n",
    "\n",
    "    def reset(\n",
    "        self, env_task: int\n",
    "    ) -> Tuple[Dict[AgentID, ObsType], Dict[AgentID, dict]]:\n",
    "        \"\"\"\n",
    "        Resets the environment and returns a dictionary of observations\n",
    "        keyed by agent ID.\n",
    "        \"\"\"\n",
    "        self.env.seed(env_task)\n",
    "        obs = self.env.reset_random()  # random level generation\n",
    "        pz_obs = self._np_array_to_pz_dict(obs[\"image\"])\n",
    "\n",
    "        return pz_obs\n",
    "\n",
    "    def step(\n",
    "        self, action: Dict[AgentID, ActionType], device: str, agent_task: int\n",
    "    ) -> Tuple[\n",
    "        Dict[AgentID, ObsType],\n",
    "        Dict[AgentID, float],\n",
    "        Dict[AgentID, bool],\n",
    "        Dict[AgentID, bool],\n",
    "        Dict[AgentID, dict],\n",
    "    ]:\n",
    "        \"\"\"\n",
    "        Takes inputs in the PettingZoo (PZ) Parallel API format, performs a step and\n",
    "        returns outputs in PZ format.\n",
    "        \"\"\"\n",
    "        action = batchify(action, device)\n",
    "        obs, rew, done, info = self.env.step(action)\n",
    "        obs = obs[\"image\"]\n",
    "        trunc = False  # there is no `truncated` flag in this environment\n",
    "        self.task_completion = self._task_completion(obs, rew, done, trunc, info)\n",
    "        # convert outputs back to PZ format\n",
    "        obs, rew = map(self._np_array_to_pz_dict, [obs, rew])\n",
    "        done, trunc, info = map(\n",
    "            self._singleton_to_pz_dict, [done, trunc, self.task_completion]\n",
    "        )\n",
    "        info[\"agent_id\"] = agent_task\n",
    "        self.n_steps += 1\n",
    "\n",
    "        return self.observation(obs), rew, done, trunc, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- convolution layer (3 Ã— 3 kernel, stride length 1, 16 filters)\n",
    "- flatten\n",
    "- Relu\n",
    "- LSTM (256)\n",
    "- Dense(32)\n",
    "- Relu\n",
    "- Dense(32)\n",
    "- Relu\n",
    "  => logits over 5 actions\n",
    "\n",
    "### TODO:\n",
    "\n",
    "**_NO AGENT DIRECTIONS AS INPUTS_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, input_channels: int, num_actions: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            self.layer_init(\n",
    "                nn.Conv2d(input_channels, out_channels=16, kernel_size=3, stride=1)\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.lstm = nn.LSTM(input_size=16 * 3 * 3, hidden_size=256, batch_first=True)\n",
    "        self.lstm_init()\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            self.layer_init(nn.Linear(256, 32)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # TODO: the paper doesn't mention the critic network?\n",
    "        self.actor = self.layer_init(nn.Linear(32, num_actions), scale=0.01)\n",
    "        self.critic = self.layer_init(nn.Linear(32, 1), scale=1)\n",
    "\n",
    "    def get_states(self, x, lstm_state, done):\n",
    "        # add batch dim if missing\n",
    "        if len(x.shape) == 3:\n",
    "            x = x.unsqueeze(0)  # shape: batch_size, *obs_shape\n",
    "\n",
    "        hidden = self.conv(x / 255.0)  # shape: batch_size, n_features, kernel, kernel\n",
    "\n",
    "        batch_size = hidden.size(0)\n",
    "        hidden = hidden.reshape(batch_size, -1)  # shape: batch_size, features\n",
    "        hidden = hidden.unsqueeze(1)  # add seq len dimension\n",
    "        # => shape: batch_size, seq_len=1, n_features\n",
    "\n",
    "        new_hidden = []\n",
    "        # reset lstm state if done\n",
    "        for h, d in zip(hidden, done):\n",
    "            h, lstm_state = self.lstm(\n",
    "                h.unsqueeze(0),\n",
    "                (\n",
    "                    torch.logical_not(d).view(1, -1, 1) * lstm_state[0],\n",
    "                    torch.logical_not(d).view(1, -1, 1) * lstm_state[1],\n",
    "                ),\n",
    "            )\n",
    "            new_hidden += [h]\n",
    "\n",
    "        new_hidden = torch.flatten(torch.cat(new_hidden), 0, 1)\n",
    "        return new_hidden, lstm_state\n",
    "\n",
    "    def get_value(self, x, lstm_state, done):\n",
    "        hidden, _ = self.get_states(x, lstm_state, done)\n",
    "        hidden = self.mlp(hidden)\n",
    "        return self.critic(hidden)\n",
    "\n",
    "    def get_action_and_value(self, x, lstm_state, done, action=None):\n",
    "        hidden, lstm_state = self.get_states(x, lstm_state, done)\n",
    "        hidden = self.mlp(hidden)\n",
    "        logits = self.actor(hidden)\n",
    "        probs = Categorical(logits=logits)\n",
    "\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "\n",
    "        return (\n",
    "            action,\n",
    "            probs.log_prob(action),\n",
    "            probs.entropy(),\n",
    "            self.critic(hidden),\n",
    "            lstm_state,\n",
    "        )\n",
    "\n",
    "    def layer_init(self, layer, scale=np.sqrt(2), bias_const=0.0):\n",
    "        torch.nn.init.orthogonal_(layer.weight, scale)\n",
    "        torch.nn.init.constant_(layer.bias, bias_const)\n",
    "        return layer\n",
    "\n",
    "    def lstm_init(self):\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if \"bias\" in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "            elif \"weight\" in name:\n",
    "                nn.init.orthogonal_(param, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_curriculums = {\n",
    "    \"SP\": SelfPlay,\n",
    "    \"FSP\": FictitiousSelfPlay,\n",
    "    \"PFSP\": PrioritizedFictitiousSelfPlay,\n",
    "}\n",
    "env_curriculums = {\n",
    "    \"DR\": DomainRandomization,\n",
    "    \"PLR\": CentralizedPrioritizedLevelReplay,\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = Args()\n",
    "    args.agent_curriculum = \"PFSP\"\n",
    "    args.env_curriculum = \"PLR\"\n",
    "    np.random.seed(args.seed)\n",
    "\n",
    "    \"\"\"ALGO PARAMS\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    stack_size = 3\n",
    "    frame_size = (5, 5)\n",
    "    n_agents = 2\n",
    "    num_actions = 5\n",
    "\n",
    "    agent_curriculum_settings = {\n",
    "        \"device\": device,\n",
    "        \"storage_path\": f\"{args.agent_curriculum}_agents\",\n",
    "        \"max_agents\": args.max_agents,\n",
    "        \"seed\": args.seed,\n",
    "    }\n",
    "\n",
    "    env_task_space = TaskSpace(spaces.Discrete(args.n_env_tasks))\n",
    "    env_curriculum_settings = {\n",
    "        \"DR\": {\"task_space\": env_task_space},\n",
    "        \"PLR\": {\n",
    "            \"task_space\": env_task_space,\n",
    "            \"num_steps\": args.rollout_length,\n",
    "            \"num_processes\": 1,  # TODO: modify if using vecenvs\n",
    "            \"gamma\": args.gamma,\n",
    "            \"gae_lambda\": args.gae_lambda,\n",
    "            \"task_sampler_kwargs_dict\": {\"strategy\": \"value_l1\"},\n",
    "        },\n",
    "    }\n",
    "\n",
    "    \"\"\" LEARNER SETUP \"\"\"\n",
    "    agent = Agent(input_channels=stack_size, num_actions=num_actions).to(device)\n",
    "    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=args.epsilon)\n",
    "\n",
    "    \"\"\" ENV SETUP \"\"\"\n",
    "    env = LasertagAdversarial(record_video=False)  # 2 agents by default\n",
    "    env = LasertagParallelWrapper(env=env, n_agents=n_agents)\n",
    "    agent_curriculum = agent_curriculums[args.agent_curriculum](\n",
    "        agent=agent, **agent_curriculum_settings\n",
    "    )\n",
    "\n",
    "    env_curriculum = env_curriculums[args.env_curriculum](\n",
    "        **env_curriculum_settings[args.env_curriculum]\n",
    "    )\n",
    "\n",
    "    curriculum = DualCurriculumWrapper(\n",
    "        env=env,\n",
    "        agent_curriculum=agent_curriculum,\n",
    "        env_curriculum=env_curriculum,\n",
    "    )\n",
    "    \n",
    "    \"\"\" ALGO LOGIC: EPISODE STORAGE\"\"\"\n",
    "    total_episodic_return = 0\n",
    "    rb_obs = torch.zeros((args.rollout_length, n_agents, stack_size, *frame_size)).to(\n",
    "        device\n",
    "    )\n",
    "    rb_actions = torch.zeros((args.rollout_length, n_agents)).to(device)\n",
    "    rb_logprobs = torch.zeros((args.rollout_length, n_agents)).to(device)\n",
    "    rb_rewards = torch.zeros((args.rollout_length, n_agents)).to(device)\n",
    "    rb_terms = torch.zeros((args.rollout_length, n_agents)).to(device)\n",
    "    rb_values = torch.zeros((args.rollout_length, n_agents)).to(device)\n",
    "\n",
    "    agent_tasks, env_tasks, rewards_history = [], [], []\n",
    "    agent_c_rew, opp_c_rew = 0, 0\n",
    "    episode, n_learner_wins = 0, 0\n",
    "    n_updates = 0\n",
    "    info = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dones = {\n",
    "    \"agent_0\": 0,\n",
    "    \"agent_1\": 0,\n",
    "}\n",
    "lstm_state = (\n",
    "    torch.zeros(agent.lstm.num_layers, 1, agent.lstm.hidden_size).to(device),\n",
    "    torch.zeros(agent.lstm.num_layers, 1, agent.lstm.hidden_size).to(device),\n",
    ")\n",
    "lstm_state_opponent = (\n",
    "    torch.zeros(agent.lstm.num_layers, 1, agent.lstm.hidden_size).to(device),\n",
    "    torch.zeros(agent.lstm.num_layers, 1, agent.lstm.hidden_size).to(device),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df07e6403dbd433e98945b9b1794b07f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 25\u001b[0m\n\u001b[0;32m     21\u001b[0m joint_obs \u001b[38;5;241m=\u001b[39m batchify(next_obs, device)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m     22\u001b[0m agent_obs, opponent_obs \u001b[38;5;241m=\u001b[39m joint_obs\n\u001b[0;32m     24\u001b[0m actions, logprobs, _, values, lstm_state \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m---> 25\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action_and_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43magent_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlstm_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatchify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdones\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     30\u001b[0m opponent \u001b[38;5;241m=\u001b[39m curriculum\u001b[38;5;241m.\u001b[39mget_opponent(info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m     31\u001b[0m     device\n\u001b[0;32m     32\u001b[0m )\n\u001b[0;32m     33\u001b[0m opponent_action, _, _, _, lstm_state_opponent \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     34\u001b[0m     opponent\u001b[38;5;241m.\u001b[39mget_action_and_value(\n\u001b[0;32m     35\u001b[0m         opponent_obs, lstm_state_opponent, batchify(dones, device)\n\u001b[0;32m     36\u001b[0m     )\n\u001b[0;32m     37\u001b[0m )\n",
      "Cell \u001b[1;32mIn[10], line 56\u001b[0m, in \u001b[0;36mAgent.get_action_and_value\u001b[1;34m(self, x, lstm_state, done, action)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_action_and_value\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, lstm_state, done, action\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 56\u001b[0m     hidden, lstm_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_states\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlstm_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden)\n\u001b[0;32m     58\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor(hidden)\n",
      "Cell \u001b[1;32mIn[10], line 28\u001b[0m, in \u001b[0;36mAgent.get_states\u001b[1;34m(self, x, lstm_state, done)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m     26\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# shape: batch_size, *obs_shape\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255.0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape: batch_size, n_features, kernel, kernel\u001b[39;00m\n\u001b[0;32m     30\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m hidden\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     31\u001b[0m hidden \u001b[38;5;241m=\u001b[39m hidden\u001b[38;5;241m.\u001b[39mreshape(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# shape: batch_size, features\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ryanp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ryanp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ryanp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ryanp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ryanp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ryanp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ryanp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "\"\"\" TRAINING LOGIC \"\"\"\n",
    "print(\"training ...\")\n",
    "with tqdm(total=args.total_updates) as pbar:\n",
    "    while n_updates < args.total_updates:\n",
    "\n",
    "        initial_lstm_state = (lstm_state[0].clone(), lstm_state[1].clone())\n",
    "        initial_lstm_state_opponent = (\n",
    "            lstm_state_opponent[0].clone(),\n",
    "            lstm_state_opponent[1].clone(),\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            env_task, agent_task = curriculum.sample()\n",
    "\n",
    "            env_tasks.append(env_task)\n",
    "            agent_tasks.append(agent_task)\n",
    "\n",
    "            next_obs = env.reset(env_task)\n",
    "            total_episodic_return = 0\n",
    "\n",
    "            for step in range(0, args.rollout_length):\n",
    "                joint_obs = batchify(next_obs, device).squeeze()\n",
    "                agent_obs, opponent_obs = joint_obs\n",
    "\n",
    "                actions, logprobs, _, values, lstm_state = (\n",
    "                    agent.get_action_and_value(\n",
    "                        agent_obs, lstm_state, batchify(dones, device)\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                opponent = curriculum.get_opponent(info.get(\"agent_id\", 0)).to(\n",
    "                    device\n",
    "                )\n",
    "                opponent_action, _, _, _, lstm_state_opponent = (\n",
    "                    opponent.get_action_and_value(\n",
    "                        opponent_obs, lstm_state_opponent, batchify(dones, device)\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                joint_actions = torch.tensor((actions, opponent_action))\n",
    "                next_obs, rewards, dones, truncs, info = env.step(\n",
    "                    unbatchify(joint_actions, env.possible_agents),\n",
    "                    device,\n",
    "                    agent_task,\n",
    "                )\n",
    "\n",
    "                opp_reward = rewards[\"agent_1\"]\n",
    "                if opp_reward != 0:\n",
    "                    curriculum.update_winrate(info[\"agent_id\"], opp_reward)\n",
    "                    if opp_reward == -1:\n",
    "                        n_learner_wins += 1\n",
    "\n",
    "                rb_obs[step] = batchify(next_obs, device)\n",
    "                rb_rewards[step] = batchify(rewards, device)\n",
    "                rb_terms[step] = batchify(dones, device)\n",
    "                rb_actions[step] = joint_actions\n",
    "                rb_logprobs[step] = logprobs\n",
    "                rb_values[step] = values.flatten()\n",
    "\n",
    "                total_episodic_return += rb_rewards[step].cpu().numpy()\n",
    "\n",
    "                agent_c_rew += rewards[\"agent_0\"]\n",
    "                opp_c_rew += rewards[\"agent_1\"]\n",
    "                grid_size = env.level[3][\"grid_size_selected\"]\n",
    "                walls_percentage = env.level[3][\"clutter_rate_selected\"]\n",
    "\n",
    "                if any([dones[a] for a in dones]) or any(\n",
    "                    [truncs[a] for a in truncs]\n",
    "                ):\n",
    "                    episode += 1\n",
    "                    rewards_history.append(rewards)\n",
    "                    env_task, agent_task = curriculum.sample()\n",
    "                    env_tasks.append(env_task)\n",
    "                    agent_tasks.append(agent_task)\n",
    "\n",
    "                    \n",
    "                   \n",
    "                    learner_winrate = n_learner_wins / episode\n",
    "\n",
    "                    next_obs = env.reset(env_task)\n",
    "\n",
    "                    # store learner checkpoints\n",
    "                    # if (\n",
    "                    #     args.save_agent_checkpoints\n",
    "                    #     and n_updates % args.checkpoint_frequency == 0\n",
    "                    # ):\n",
    "                    #     print(f\"saving checkpoint --{n_updates}\")\n",
    "                    #     checkpoint_path = (\n",
    "                    #         f\"{args.logging_dir}/{run_name}_checkpoints/\"\n",
    "                    #         f\"{curriculum.env_curriculum.name}_\"\n",
    "                    #         f\"{curriculum.agent_curriculum.name}_{n_updates}\"\n",
    "                    #         f\"_seed_{args.seed}.pkl\"\n",
    "                    #     )\n",
    "                    #     # --- local checkpoint ---\n",
    "                    #     joblib.dump(\n",
    "                    #         agent,\n",
    "                    #         filename=checkpoint_path,\n",
    "                    #     )\n",
    "\n",
    "                        # --- wandb checkpoint ---\n",
    "                        # if args.track:\n",
    "                        #     agent_artifact = wandb.Artifact(\"model\", type=\"model\")\n",
    "                        #     agent_artifact.add_file(checkpoint_path)\n",
    "                        #     wandb.log_artifact(agent_artifact)\n",
    "\n",
    "            if args.env_curriculum == \"PLR\":\n",
    "                with torch.no_grad():\n",
    "                    next_value = agent.get_value(\n",
    "                        torch.tensor(next_obs[\"agent_0\"]).to(device),\n",
    "                        lstm_state,\n",
    "                        batchify(dones, device),\n",
    "                    )\n",
    "                update = {\n",
    "                    \"update_type\": \"on_demand\",\n",
    "                    \"metrics\": {\n",
    "                        \"value\": values,\n",
    "                        \"next_value\": next_value,\n",
    "                        \"rew\": rewards[\n",
    "                            \"agent_0\"\n",
    "                        ],  # TODO: is this the expected use?\n",
    "                        \"dones\": dones[0],\n",
    "                        \"tasks\": env_task,\n",
    "                    },\n",
    "                }\n",
    "\n",
    "        # gae\n",
    "        with torch.no_grad():\n",
    "            next_value = agent.get_value(\n",
    "                torch.tensor(next_obs[\"agent_0\"]).to(device),\n",
    "                lstm_state,\n",
    "                batchify(dones, device),\n",
    "            )\n",
    "            rb_advantages = torch.zeros_like(rb_rewards).to(device)\n",
    "            last_gae_lam = 0\n",
    "            for t in reversed(range(args.rollout_length - 1)):\n",
    "                if t == args.rollout_length - 1:\n",
    "                    next_non_terminal = 1.0 - rb_terms[t + 1]\n",
    "                    next_values = next_value\n",
    "                else:\n",
    "                    next_non_terminal = 1.0 - rb_terms[t + 1]\n",
    "                    next_values = rb_values[t + 1]\n",
    "                delta = (\n",
    "                    rb_rewards[t]\n",
    "                    + args.gamma * next_values * next_non_terminal\n",
    "                    - rb_values[t]\n",
    "                )\n",
    "                rb_advantages[t] = last_gae_lam = (\n",
    "                    delta\n",
    "                    + args.gamma\n",
    "                    * args.gae_lambda\n",
    "                    * next_non_terminal\n",
    "                    * last_gae_lam\n",
    "                )\n",
    "            rb_returns = rb_advantages + rb_values\n",
    "\n",
    "        b_obs = torch.flatten(rb_obs[: args.rollout_length], start_dim=0, end_dim=1)\n",
    "        b_logprobs = torch.flatten(\n",
    "            rb_logprobs[: args.rollout_length], start_dim=0, end_dim=1\n",
    "        )\n",
    "        b_actions = torch.flatten(\n",
    "            rb_actions[: args.rollout_length], start_dim=0, end_dim=1\n",
    "        )\n",
    "        b_returns = torch.flatten(\n",
    "            rb_returns[: args.rollout_length], start_dim=0, end_dim=1\n",
    "        )\n",
    "        b_values = torch.flatten(\n",
    "            rb_values[: args.rollout_length], start_dim=0, end_dim=1\n",
    "        )\n",
    "        b_advantages = torch.flatten(\n",
    "            rb_advantages[: args.rollout_length], start_dim=0, end_dim=1\n",
    "        )\n",
    "        b_terms = torch.flatten(\n",
    "            rb_terms[: args.rollout_length], start_dim=0, end_dim=1\n",
    "        )\n",
    "\n",
    "        b_index = np.arange(len(b_obs))\n",
    "        clip_fracs = []\n",
    "        for repeat in range(args.epochs):\n",
    "            np.random.shuffle(b_index)\n",
    "            for start in range(0, len(b_obs), args.batch_size):\n",
    "                # select the indices we want to train on\n",
    "                end = start + args.batch_size\n",
    "                batch_index = b_index[start:end]\n",
    "\n",
    "                _, newlogprob, entropy, value, _ = agent.get_action_and_value(\n",
    "                    b_obs[batch_index],\n",
    "                    (\n",
    "                        initial_lstm_state[0],\n",
    "                        initial_lstm_state[1],\n",
    "                    ),\n",
    "                    b_terms[batch_index],\n",
    "                    b_actions.long()[batch_index],\n",
    "                )\n",
    "                logratio = newlogprob - b_logprobs[batch_index]\n",
    "                ratio = logratio.exp()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                    old_approx_kl = (-logratio).mean()\n",
    "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                    clip_fracs += [\n",
    "                        ((ratio - 1.0).abs() > args.clip_coef).float().mean().item()\n",
    "                    ]\n",
    "\n",
    "                # normalize advantages\n",
    "                rb_advantages = b_advantages[batch_index]\n",
    "                rb_advantages = (rb_advantages - rb_advantages.mean()) / (\n",
    "                    rb_advantages.std() + 1e-8\n",
    "                )\n",
    "\n",
    "                # Policy loss\n",
    "                pg_loss1 = -b_advantages[batch_index] * ratio\n",
    "                pg_loss2 = -b_advantages[batch_index] * torch.clamp(\n",
    "                    ratio, 1 - args.clip_coef, 1 + args.clip_coef\n",
    "                )\n",
    "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                # Value loss\n",
    "                value = value.flatten()\n",
    "                v_loss_unclipped = (value - b_returns[batch_index]) ** 2\n",
    "                v_clipped = b_values[batch_index] + torch.clamp(\n",
    "                    value - b_values[batch_index],\n",
    "                    -args.clip_coef,\n",
    "                    args.clip_coef,\n",
    "                )\n",
    "                v_loss_clipped = (v_clipped - b_returns[batch_index]) ** 2\n",
    "                v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                v_loss = 0.5 * v_loss_max.mean()\n",
    "\n",
    "                entropy_loss = entropy.mean()\n",
    "                loss = (\n",
    "                    pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n",
    "                )\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # writer.add_scalar(\"losses/value_loss\", v_loss.item(), n_updates)\n",
    "                # writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), n_updates)\n",
    "                # writer.add_scalar(\"losses/entropy\", entropy_loss.item(), n_updates)\n",
    "                # writer.add_scalar(\n",
    "                #     \"losses/old_approx_kl\", old_approx_kl.item(), n_updates\n",
    "                # )\n",
    "                # writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), n_updates)\n",
    "                n_updates += 1\n",
    "                pbar.update(1)\n",
    "\n",
    "                # update opponent\n",
    "                if args.agent_curriculum in [\"FSP\", \"PFSP\"]:\n",
    "                    if (\n",
    "                        n_updates % args.agent_update_frequency == 0\n",
    "                        and episode != 0\n",
    "                    ):\n",
    "                        curriculum.update_agent(agent)\n",
    "\n",
    "        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "        var_y = np.var(y_true)\n",
    "        explained_var = (\n",
    "            np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': False, '1': False}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'curriculum' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcurriculum\u001b[49m\u001b[38;5;241m.\u001b[39magent_curriculum\u001b[38;5;241m.\u001b[39mname\n",
      "\u001b[1;31mNameError\u001b[0m: name 'curriculum' is not defined"
     ]
    }
   ],
   "source": [
    "curriculum.agent_curriculum.name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
