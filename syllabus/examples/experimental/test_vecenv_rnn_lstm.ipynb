{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (module.py, line 31)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\ryanp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3548\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[0;32mIn[1], line 10\u001b[0m\n    import torch\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\ryanp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\__init__.py:1537\u001b[0m\n    from .functional import *  # noqa: F403\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\ryanp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\functional.py:9\u001b[0m\n    import torch.nn.functional as F\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\ryanp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\__init__.py:1\u001b[0m\n    from .modules import *  # noqa: F403\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32mc:\\Users\\ryanp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\__init__.py:1\u001b[1;36m\n\u001b[1;33m    from .module import Module\u001b[1;36m\n",
      "\u001b[1;36m  File \u001b[1;32mc:\\Users\\ryanp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:31\u001b[1;36m\u001b[0m\n\u001b[1;33m    Union,\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/ppo/#ppo_pettingzoo_ma_ataripy\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple, TypeVar\n",
    "\n",
    "import gymnasium\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tyro\n",
    "from colorama import Fore, Style, init\n",
    "from pufferlib.emulation import PettingZooPufferEnv\n",
    "from pufferlib.vector import Serial\n",
    "from torch.distributions.categorical import Categorical\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(\"../../..\")\n",
    "from lasertag import LasertagAdversarial  # noqa: E402\n",
    "from syllabus.core import PettingZooTaskWrapper\n",
    "\n",
    "ActionType = TypeVar(\"ActionType\")\n",
    "AgentID = TypeVar(\"AgentID\")\n",
    "AgentType = TypeVar(\"AgentType\")\n",
    "EnvTask = TypeVar(\"EnvTask\")\n",
    "AgentTask = TypeVar(\"AgentTask\")\n",
    "ObsType = TypeVar(\"ObsType\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # Experiment configuration\n",
    "    exp_name: str = \"lasertag_\"\n",
    "    wandb_project_name: str = \"syllabus\"\n",
    "    wandb_entity: str = \"rpegoud\"\n",
    "    logging_dir: str = \".\"\n",
    "    torch_deterministic: bool = True\n",
    "    cuda: bool = True\n",
    "    track: bool = False\n",
    "    capture_video: bool = False\n",
    "\n",
    "    # Experiment setup\n",
    "    env_id: str = \"pong_v3\"\n",
    "    total_timesteps: int = int(1e5)\n",
    "    seed: int = 0\n",
    "\n",
    "    # Algorithm specific arguments\n",
    "    gamma: float = 0.995\n",
    "    gae_lambda: float = 0.95\n",
    "    anneal_lr: bool = True\n",
    "    norm_adv: bool = True\n",
    "    target_kl = None  # type: float\n",
    "    # Adam args\n",
    "    adam_lr: float = 1e-4\n",
    "    adam_eps: float = 1e-5\n",
    "    # PPO args\n",
    "    clip_coef: float = 0.2\n",
    "    vf_coef: float = 0.5\n",
    "    clip_vloss: bool = True\n",
    "    max_grad_norm: float = 0.5\n",
    "    ent_coef: float = 0.0\n",
    "    num_workers: int = 8\n",
    "    num_minibatches: int = 4\n",
    "    rollout_length: int = 256\n",
    "    update_epochs: int = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_args() -> Config:\n",
    "    init(autoreset=True)  # Initialize colorama\n",
    "    config = tyro.cli(Config)\n",
    "    config.batch_size = int(config.num_workers * config.rollout_length)\n",
    "    print(\n",
    "        f\"{Fore.RED}{Style.BRIGHT} Setting BATCH_SIZE to: NUM_WORKERS * ROLLOUT_LENGTH\"\n",
    "        f\"= {config.batch_size}\"\n",
    "    )\n",
    "    config.minibatch_size = int(config.batch_size // config.num_minibatches)\n",
    "    print(\n",
    "        f\"{Fore.RED}{Style.BRIGHT} Setting MINIBATCH_SIZE to:\"\n",
    "        f\"BATCH_SIZE * NUM_MINIBATCHES= {config.minibatch_size}\"\n",
    "    )\n",
    "    log_config(config)\n",
    "    print(\n",
    "        f\"{Fore.BLUE}{Style.BRIGHT} RUNNING PPO ON {config.env_id} \"\n",
    "        f\"FOR {config.total_timesteps} TIMESTEPS ...\"\n",
    "    )\n",
    "    return config\n",
    "\n",
    "\n",
    "def log_config(config: Config) -> None:\n",
    "    config_dict = config.__dict__\n",
    "    print(f\"{Fore.GREEN}{Style.BRIGHT} Config:\")\n",
    "    log_str = \" \\n\".join(\n",
    "        [\n",
    "            f\"{key.replace('_', ' ').capitalize()}: {Fore.GREEN}{value}{Style.RESET_ALL}\"\n",
    "            for key, value in config_dict.items()\n",
    "        ]\n",
    "    )\n",
    "    print(log_str)\n",
    "\n",
    "\n",
    "def log_rewards(r_batch: torch.tensor, run, step: int) -> None:\n",
    "    r_logs = {f\"agent_{agent_idx}\": reward for agent_idx, reward in enumerate(r_batch)}\n",
    "\n",
    "    run[\"charts/cumulative_rewards_per_update\"].append(r_logs, step=step)\n",
    "\n",
    "\n",
    "def batchify(x, device=None):\n",
    "    \"\"\"Converts PZ style returns to batch of torch arrays.\"\"\"\n",
    "    # convert to list of np arrays\n",
    "    x = np.stack([x[a] for a in x], axis=0)\n",
    "    # convert to torch\n",
    "    x = torch.tensor(x)\n",
    "\n",
    "    if device is not None:\n",
    "        x = x.to(device)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def unbatchify(x, possible_agents: np.ndarray):\n",
    "    \"\"\"Converts np array to PZ style arguments.\"\"\"\n",
    "    x = x.cpu().numpy()\n",
    "    x = {agent: x[idx] for idx, agent in enumerate(possible_agents)}\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "class LasertagParallelWrapper(PettingZooTaskWrapper):\n",
    "    \"\"\"\n",
    "    Wrapper ensuring compatibility with the PettingZoo Parallel API.\n",
    "\n",
    "    Lasertag Environment:\n",
    "        * Action shape:  `n_agents` * `Discrete(5)`\n",
    "        * Observation shape: Dict('image': Box(0, 255, (`n_agents`, 3, 5, 5), uint8))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_agents, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.n_agents = n_agents\n",
    "        self.task = None\n",
    "        self.episode_return = 0\n",
    "        self.possible_agents = [f\"agent_{i}\" for i in range(self.n_agents)]\n",
    "        self.env.agents = self.possible_agents\n",
    "        self.n_steps = 0\n",
    "        self.env.render_mode = \"human\"\n",
    "\n",
    "    def observation_space(self, agent):\n",
    "        env_space = self.env.observation_space[\"image\"]\n",
    "        # Remove agent dimension\n",
    "        return gymnasium.spaces.Box(\n",
    "            low=env_space.low[0],\n",
    "            high=env_space.high[0],\n",
    "            shape=env_space.shape[1:],\n",
    "            dtype=env_space.dtype,\n",
    "        )\n",
    "\n",
    "    def action_space(self, agent):\n",
    "        return gymnasium.spaces.Discrete(5)\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        \"\"\"\n",
    "        Delegate attribute lookup to the wrapped environment if the attribute\n",
    "        is not found in the LasertagParallelWrapper instance.\n",
    "        \"\"\"\n",
    "        return getattr(self.env, name)\n",
    "\n",
    "    def _np_array_to_pz_dict(self, array: np.ndarray) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Returns a dictionary containing individual observations for each agent.\n",
    "        Assumes that the batch dimension represents individual agents.\n",
    "        \"\"\"\n",
    "        out = {}\n",
    "        for idx, value in enumerate(array):\n",
    "            out[self.possible_agents[idx]] = value\n",
    "        return out\n",
    "\n",
    "    def _singleton_to_pz_dict(self, value: bool) -> Dict[str, bool]:\n",
    "        \"\"\"\n",
    "        Broadcasts the `done` and `trunc` flags to dictionaries keyed by agent id.\n",
    "        \"\"\"\n",
    "        return {agent: value for agent in self.agents}\n",
    "\n",
    "    def reset(\n",
    "        self, seed: int = None\n",
    "    ) -> Tuple[Dict[AgentID, ObsType], Dict[AgentID, dict]]:\n",
    "        \"\"\"\n",
    "        Resets the environment and returns a dictionary of observations\n",
    "        keyed by agent ID.\n",
    "        \"\"\"\n",
    "        self.env.seed(seed)\n",
    "        obs = self.env.reset_random()  # random level generation\n",
    "        pz_obs = self._np_array_to_pz_dict(obs[\"image\"])\n",
    "        return pz_obs, {}\n",
    "\n",
    "    def step(\n",
    "        self,\n",
    "        action: Dict[AgentID, ActionType],\n",
    "    ) -> Tuple[\n",
    "        Dict[AgentID, ObsType],\n",
    "        Dict[AgentID, float],\n",
    "        Dict[AgentID, bool],\n",
    "        Dict[AgentID, bool],\n",
    "        Dict[AgentID, dict],\n",
    "    ]:\n",
    "        \"\"\"\n",
    "        Takes inputs in the PettingZoo (PZ) Parallel API format, performs a step and\n",
    "        returns outputs in PZ format.\n",
    "        \"\"\"\n",
    "        action = batchify(action)\n",
    "        obs, rew, done, info = self.env.step(action)\n",
    "        obs = obs[\"image\"]\n",
    "        trunc = False  # there is no `truncated` flag in this environment\n",
    "        self.task_completion = self._task_completion(obs, rew, done, trunc, info)\n",
    "        # convert outputs back to PZ format\n",
    "        obs, rew = map(self._np_array_to_pz_dict, [obs, rew])\n",
    "        done, trunc, info = map(\n",
    "            self._singleton_to_pz_dict, [done, trunc, self.task_completion]\n",
    "        )\n",
    "        # info[\"agent_id\"] = agent_task\n",
    "        self.n_steps += 1\n",
    "        return self.observation(obs), rew, done, trunc, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    env = LasertagAdversarial()\n",
    "    env = LasertagParallelWrapper(env=env, n_agents=2)\n",
    "    env = PettingZooPufferEnv(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            self.layer_init(\n",
    "                nn.Conv2d(\n",
    "                    envs.single_observation_space.shape[0],\n",
    "                    out_channels=16,\n",
    "                    kernel_size=3,\n",
    "                    stride=1,\n",
    "                )\n",
    "            ),\n",
    "            nn.Flatten(),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.lstm = nn.LSTM(input_size=16 * 3 * 3, hidden_size=256, batch_first=True)\n",
    "        self.lstm_init()\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            self.layer_init(nn.Linear(256, 32)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.actor = self.layer_init(\n",
    "            nn.Linear(32, envs.single_action_space.n), scale=0.01\n",
    "        )\n",
    "        self.critic = self.layer_init(nn.Linear(32, 1), scale=1)\n",
    "\n",
    "    def get_states(self, x, lstm_states, done):\n",
    "        # x shape: (num_envs, *obs_shape)\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.conv(x / 255.0)\n",
    "        hidden = hidden.view(batch_size, 1, -1)  # shape: (num_envs, 1, n_features)\n",
    "\n",
    "        # Reshape LSTM states\n",
    "        h_states, c_states = lstm_states\n",
    "        h_states = h_states.view(1, batch_size, -1)\n",
    "        c_states = c_states.view(1, batch_size, -1)\n",
    "\n",
    "        # Apply the LSTM\n",
    "        hidden, (new_h_states, new_c_states) = self.lstm(hidden, (h_states, c_states))\n",
    "\n",
    "        # Reset LSTM state if done\n",
    "        done = done.view(batch_size, 1, 1)\n",
    "        new_h_states = new_h_states * (1 - done)\n",
    "        new_c_states = new_c_states * (1 - done)\n",
    "\n",
    "        hidden = hidden.view(batch_size, -1)\n",
    "        new_lstm_states = (\n",
    "            new_h_states.view(batch_size, -1),\n",
    "            new_c_states.view(batch_size, -1),\n",
    "        )\n",
    "\n",
    "        return hidden, new_lstm_states\n",
    "\n",
    "    def get_value(self, x, lstm_states, done):\n",
    "        hidden, _ = self.get_states(x, lstm_states, done)\n",
    "        hidden = self.mlp(hidden)\n",
    "        return self.critic(hidden)\n",
    "\n",
    "    def get_action_and_value(self, x, lstm_states, done, action=None):\n",
    "        hidden, new_lstm_states = self.get_states(x, lstm_states, done)\n",
    "        hidden = self.mlp(hidden)\n",
    "        logits = self.actor(hidden)\n",
    "        probs = Categorical(logits=logits)\n",
    "\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "\n",
    "        return (\n",
    "            action,\n",
    "            probs.log_prob(action),\n",
    "            probs.entropy(),\n",
    "            self.critic(hidden),\n",
    "            new_lstm_states,\n",
    "        )\n",
    "\n",
    "    def layer_init(self, layer, scale=np.sqrt(2), bias_const=0.0):\n",
    "        torch.nn.init.orthogonal_(layer.weight, scale)\n",
    "        torch.nn.init.constant_(layer.bias, bias_const)\n",
    "        return layer\n",
    "\n",
    "    def lstm_init(self):\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if \"bias\" in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "            elif \"weight\" in name:\n",
    "                nn.init.orthogonal_(param, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(5)\n",
      "Box(0, 255, (3, 5, 5), uint8)\n"
     ]
    }
   ],
   "source": [
    "args = Config()\n",
    "args.batch_size = int(args.num_workers * args.rollout_length)\n",
    "args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# env setup\n",
    "envs = [make_env for _ in range(args.num_workers)]\n",
    "envs = Serial(\n",
    "    envs,\n",
    "    [() for _ in range(args.num_workers)],\n",
    "    [{} for _ in range(args.num_workers)],\n",
    "    args.num_workers,\n",
    ")\n",
    "print(envs.single_action_space)\n",
    "print(envs.single_observation_space)\n",
    "envs.is_vector_env = True\n",
    "\n",
    "agent = Agent(envs).to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=args.adam_lr, eps=args.adam_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = torch.zeros(\n",
    "    (args.rollout_length, args.num_workers * 2)\n",
    "    + envs.single_observation_space.shape\n",
    ").to(device)\n",
    "actions = torch.zeros(\n",
    "    (args.rollout_length, args.num_workers * 2) + envs.single_action_space.shape\n",
    ").to(device)\n",
    "logprobs = torch.zeros((args.rollout_length, args.num_workers * 2)).to(device)\n",
    "rewards = torch.zeros((args.rollout_length, args.num_workers * 2)).to(device)\n",
    "dones = torch.zeros((args.rollout_length, args.num_workers * 2)).to(device)\n",
    "values = torch.zeros((args.rollout_length, args.num_workers * 2)).to(device)\n",
    "\n",
    "lstm_state = (\n",
    "    torch.zeros(agent.lstm.num_layers, args.num_workers, agent.lstm.hidden_size).to(device),\n",
    "    torch.zeros(agent.lstm.num_layers, args.num_workers, agent.lstm.hidden_size).to(device),\n",
    ")\n",
    "lstm_state_opponent = (\n",
    "    torch.zeros(agent.lstm.num_layers, args.num_workers, agent.lstm.hidden_size).to(device),\n",
    "    torch.zeros(agent.lstm.num_layers, args.num_workers, agent.lstm.hidden_size).to(device),\n",
    ")\n",
    "\n",
    "# TRY NOT TO MODIFY: start the game\n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "next_obs, info = envs.reset()\n",
    "next_obs = torch.Tensor(next_obs).to(device)\n",
    "next_done = torch.zeros(args.num_workers * 2).to(device)\n",
    "num_updates = int(args.total_timesteps // args.batch_size)\n",
    "\n",
    "cumulative_rewards = np.zeros(args.num_workers * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([16, 3, 5, 5]), torch.Size([1, 8, 256]), torch.Size([16])]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: x.shape, (next_obs, lstm_state[0], next_done)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_batch(joint_obs: torch.Tensor) -> Tuple[torch.Tensor]:\n",
    "    \"\"\"Splits a batch of joint data in agent and opponent data.\"\"\"\n",
    "    agent_data = joint_obs[[i for i in np.arange(0, args.num_workers * 2, 2)]]\n",
    "    opponent_data = joint_obs[[i for i in np.arange(1, args.num_workers * 2, 2)]]\n",
    "    return agent_data, opponent_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[133], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m agent_obs, opponent_obs \u001b[38;5;241m=\u001b[39m split_batch(next_obs)\n\u001b[1;32m----> 2\u001b[0m agent_done, opponent_done \u001b[38;5;241m=\u001b[39m \u001b[43msplit_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_done\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m agent\u001b[38;5;241m.\u001b[39mget_action_and_value(agent_obs, lstm_state, agent_done)\n",
      "Cell \u001b[1;32mIn[127], line 4\u001b[0m, in \u001b[0;36msplit_batch\u001b[1;34m(joint_obs)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Splits a batch of joint data in agent and opponent data.\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m agent_data \u001b[38;5;241m=\u001b[39m joint_obs[[i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, args\u001b[38;5;241m.\u001b[39mnum_workers \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)]]\n\u001b[1;32m----> 4\u001b[0m opponent_data \u001b[38;5;241m=\u001b[39m \u001b[43mjoint_obs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m agent_data, opponent_data\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "agent_obs, opponent_obs = split_batch(next_obs)\n",
    "agent_done, opponent_done = split_batch(next_done)\n",
    "\n",
    "agent.get_action_and_value(agent_obs, lstm_state, agent_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/48 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden[0] size (1, 16, 256), got [1, 16, 4096]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ALGO LOGIC: action logic\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 16\u001b[0m     action, logprob, _, value, lstm_state \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action_and_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnext_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlstm_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_done\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     values[step] \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     20\u001b[0m actions[step] \u001b[38;5;241m=\u001b[39m action\n",
      "Cell \u001b[1;32mIn[5], line 63\u001b[0m, in \u001b[0;36mAgent.get_action_and_value\u001b[1;34m(self, x, lstm_states, done, action)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_action_and_value\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, lstm_states, done, action\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 63\u001b[0m     hidden, new_lstm_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_states\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlstm_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden)\n\u001b[0;32m     65\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor(hidden)\n",
      "Cell \u001b[1;32mIn[5], line 42\u001b[0m, in \u001b[0;36mAgent.get_states\u001b[1;34m(self, x, lstm_states, done)\u001b[0m\n\u001b[0;32m     39\u001b[0m c_states \u001b[38;5;241m=\u001b[39m c_states\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Apply the LSTM\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m hidden, (new_h_states, new_c_states) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Reset LSTM state if done\u001b[39;00m\n\u001b[0;32m     45\u001b[0m done \u001b[38;5;241m=\u001b[39m done\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ryanp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ryanp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ryanp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:907\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    904\u001b[0m             hx \u001b[38;5;241m=\u001b[39m (hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[0;32m    906\u001b[0m         \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[1;32m--> 907\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    908\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m    910\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ryanp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:822\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\u001b[38;5;28mself\u001b[39m,  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m    817\u001b[0m                        \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[0;32m    818\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[0;32m    819\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[0;32m    820\u001b[0m                        ):\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_input(\u001b[38;5;28minput\u001b[39m, batch_sizes)\n\u001b[1;32m--> 822\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_hidden_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_expected_hidden_size\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    823\u001b[0m \u001b[43m                           \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mExpected hidden[0] size \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m, got \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m    825\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ryanp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:260\u001b[0m, in \u001b[0;36mRNNBase.check_hidden_size\u001b[1;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_hidden_size\u001b[39m(\u001b[38;5;28mself\u001b[39m, hx: Tensor, expected_hidden_size: Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m],\n\u001b[0;32m    258\u001b[0m                       msg: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hx\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m expected_hidden_size:\n\u001b[1;32m--> 260\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(expected_hidden_size, \u001b[38;5;28mlist\u001b[39m(hx\u001b[38;5;241m.\u001b[39msize())))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected hidden[0] size (1, 16, 256), got [1, 16, 4096]"
     ]
    }
   ],
   "source": [
    "with tqdm(total=num_updates) as pbar:\n",
    "    for update in range(1, num_updates + 1):\n",
    "        # Annealing the rate if instructed to do so.\n",
    "        if args.anneal_lr:\n",
    "            frac = 1.0 - (update - 1.0) / num_updates\n",
    "            lrnow = frac * args.adam_lr\n",
    "            optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "        for step in range(0, args.rollout_length):\n",
    "            global_step += 1 * args.num_workers * 2\n",
    "            obs[step] = next_obs\n",
    "            dones[step] = next_done\n",
    "\n",
    "            # ALGO LOGIC: action logic\n",
    "            with torch.no_grad():\n",
    "                action, logprob, _, value, lstm_state = agent.get_action_and_value(\n",
    "                    next_obs, lstm_state, next_done\n",
    "                )\n",
    "                values[step] = value.flatten()\n",
    "            actions[step] = action\n",
    "            logprobs[step] = logprob\n",
    "\n",
    "            # TRY NOT TO MODIFY: execute the game and log data.\n",
    "            next_obs, reward, done, trunc, info = envs.step(action.cpu().numpy())\n",
    "            rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "            next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(\n",
    "                done\n",
    "            ).to(device)\n",
    "\n",
    "            cumulative_rewards = cumulative_rewards + reward\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_value = agent.get_value(next_obs, lstm_state, next_done).reshape(1, -1)\n",
    "            advantages = torch.zeros_like(rewards).to(device)\n",
    "            lastgaelam = 0\n",
    "            for t in reversed(range(args.rollout_length)):\n",
    "                if t == args.rollout_length - 1:\n",
    "                    nextnonterminal = 1.0 - next_done\n",
    "                    nextvalues = next_value\n",
    "                else:\n",
    "                    nextnonterminal = 1.0 - dones[t + 1]\n",
    "                    nextvalues = values[t + 1]\n",
    "                delta = (\n",
    "                    rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n",
    "                )\n",
    "                advantages[t] = lastgaelam = (\n",
    "                    delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n",
    "                )\n",
    "            returns = advantages + values\n",
    "\n",
    "        # flatten the batch\n",
    "        b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "        b_dones = dones.reshape((-1,))\n",
    "        b_logprobs = logprobs.reshape(-1)\n",
    "        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "        b_advantages = advantages.reshape(-1)\n",
    "        b_returns = returns.reshape(-1)\n",
    "        b_values = values.reshape(-1)\n",
    "\n",
    "        # Optimizing the policy and value network\n",
    "        b_inds = np.arange(args.batch_size)\n",
    "        clipfracs = []\n",
    "        for epoch in range(args.update_epochs):\n",
    "            np.random.shuffle(b_inds)\n",
    "            for start in range(0, args.batch_size, args.minibatch_size):\n",
    "                end = start + args.minibatch_size\n",
    "                mb_inds = b_inds[start:end]\n",
    "\n",
    "                _, newlogprob, entropy, newvalue, lstm_state = (\n",
    "                    agent.get_action_and_value(\n",
    "                        b_obs[mb_inds],\n",
    "                        lstm_state,\n",
    "                        b_dones,\n",
    "                        b_actions.long()[mb_inds],\n",
    "                    )\n",
    "                )\n",
    "                logratio = newlogprob - b_logprobs[mb_inds]\n",
    "                ratio = logratio.exp()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                    old_approx_kl = (-logratio).mean()\n",
    "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                    clipfracs += [\n",
    "                        ((ratio - 1.0).abs() > args.clip_coef).float().mean().item()\n",
    "                    ]\n",
    "\n",
    "                mb_advantages = b_advantages[mb_inds]\n",
    "                if args.norm_adv:\n",
    "                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (\n",
    "                        mb_advantages.std() + 1e-8\n",
    "                    )\n",
    "\n",
    "                # Policy loss\n",
    "                pg_loss1 = -mb_advantages * ratio\n",
    "                pg_loss2 = -mb_advantages * torch.clamp(\n",
    "                    ratio, 1 - args.clip_coef, 1 + args.clip_coef\n",
    "                )\n",
    "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                # Value loss\n",
    "                newvalue = newvalue.view(-1)\n",
    "                if args.clip_vloss:\n",
    "                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "                    v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "                        newvalue - b_values[mb_inds],\n",
    "                        -args.clip_coef,\n",
    "                        args.clip_coef,\n",
    "                    )\n",
    "                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                    v_loss = 0.5 * v_loss_max.mean()\n",
    "                else:\n",
    "                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "                entropy_loss = entropy.mean()\n",
    "                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "\n",
    "            if args.target_kl is not None:\n",
    "                if approx_kl > args.target_kl:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
