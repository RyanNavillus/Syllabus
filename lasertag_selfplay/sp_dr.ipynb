{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from typing import TypeVar,Any, Callable, Dict, Tuple\n",
    "from gymnasium.utils.step_api_compatibility import step_api_compatibility\n",
    "from multiprocessing import SimpleQueue\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "from gymnasium import spaces\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from lasertag import LasertagAdversarial  # noqa: E402\n",
    "from syllabus.core import (\n",
    "    Curriculum,\n",
    "    TaskWrapper,\n",
    "    make_multiprocessing_curriculum,\n",
    "    MultiProcessingSyncWrapper,\n",
    "    MultiProcessingCurriculumWrapper,\n",
    ")  # noqa: E402\n",
    "from syllabus.curricula import DomainRandomization  # noqa: E402\n",
    "from syllabus.task_space import TaskSpace  # noqa: E402\n",
    "\n",
    "ObsType = TypeVar(\"ObsType\")\n",
    "ActionType = TypeVar(\"ActionType\")\n",
    "AgentID = TypeVar(\"AgentID\")\n",
    "Agent = TypeVar(\"Agent\")\n",
    "EnvTask = TypeVar(\"EnvTask\")\n",
    "AgentTask = TypeVar(\"AgentTask\")\n",
    "\n",
    "\n",
    "def batchify(x, device):\n",
    "    \"\"\"Converts PZ style returns to batch of torch arrays.\"\"\"\n",
    "    # convert to list of np arrays\n",
    "    x = np.stack([x[a] for a in x], axis=0)\n",
    "    # convert to torch\n",
    "    x = torch.tensor(x).to(device)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def unbatchify(x, possible_agents: np.ndarray):\n",
    "    \"\"\"Converts np array to PZ style arguments.\"\"\"\n",
    "    x = x.cpu().numpy()\n",
    "    x = {agent: x[idx] for idx, agent in enumerate(possible_agents)}\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "class LasertagParallelWrapper(TaskWrapper):\n",
    "    \"\"\"\n",
    "    Wrapper ensuring compatibility with the PettingZoo Parallel API.\n",
    "\n",
    "    Lasertag Environment:\n",
    "        * Action shape:  `n_agents` * `Discrete(5)`\n",
    "        * Observation shape: Dict('image': Box(0, 255, (`n_agents`, 3, 5, 5), uint8))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_agents, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.n_agents = n_agents\n",
    "        self.task = None\n",
    "        self.episode_return = 0\n",
    "        # self.task_space = TaskSpace(spaces.MultiDiscrete(np.array([[2], [5]])))\n",
    "        self.possible_agents = [f\"agent_{i}\" for i in range(self.n_agents)]\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        \"\"\"\n",
    "        Delegate attribute lookup to the wrapped environment if the attribute\n",
    "        is not found in the LasertagParallelWrapper instance.\n",
    "        \"\"\"\n",
    "        return getattr(self.env, name)\n",
    "\n",
    "    def _np_array_to_pz_dict(self, array: np.ndarray) -> dict[str : np.ndarray]:\n",
    "        \"\"\"\n",
    "        Returns a dictionary containing individual observations for each agent.\n",
    "        Assumes that the batch dimension represents individual agents.\n",
    "        \"\"\"\n",
    "        out = {}\n",
    "        for idx, value in enumerate(array):\n",
    "            out[self.possible_agents[idx]] = value\n",
    "        return out\n",
    "\n",
    "    def _singleton_to_pz_dict(self, value: bool) -> dict[str:bool]:\n",
    "        \"\"\"\n",
    "        Broadcasts the `done` and `trunc` flags to dictionaries keyed by agent id.\n",
    "        \"\"\"\n",
    "        return {str(agent_index): value for agent_index in range(self.n_agents)}\n",
    "\n",
    "    def reset(self, env_task:int) -> tuple[dict[AgentID, ObsType], dict[AgentID, dict]]:\n",
    "        \"\"\"\n",
    "        Resets the environment and returns a dictionary of observations\n",
    "        keyed by agent ID.\n",
    "        \"\"\"\n",
    "        self.env.seed(env_task)\n",
    "        obs = self.env.reset_random()  # random level generation\n",
    "        pz_obs = self._np_array_to_pz_dict(obs[\"image\"])\n",
    "\n",
    "        return pz_obs\n",
    "\n",
    "    def step(\n",
    "        self, action: dict[AgentID, ActionType], device: str, agent_task: int\n",
    "    ) -> tuple[\n",
    "        dict[AgentID, ObsType],\n",
    "        dict[AgentID, float],\n",
    "        dict[AgentID, bool],\n",
    "        dict[AgentID, bool],\n",
    "        dict[AgentID, dict],\n",
    "    ]:\n",
    "        \"\"\"\n",
    "        Takes inputs in the PettingZoo (PZ) Parallel API format, performs a step and\n",
    "        returns outputs in PZ format.\n",
    "        \"\"\"\n",
    "        action = batchify(action, device)\n",
    "        obs, rew, done, info = self.env.step(action)\n",
    "        obs = obs[\"image\"]\n",
    "        trunc = False  # there is no `truncated` flag in this environment\n",
    "        self.task_completion = self._task_completion(obs, rew, done, trunc, info)\n",
    "        # convert outputs back to PZ format\n",
    "        obs, rew = map(self._np_array_to_pz_dict, [obs, rew])\n",
    "        done, trunc, info = map(\n",
    "            self._singleton_to_pz_dict, [done, trunc, self.task_completion]\n",
    "        )\n",
    "        info[\"agent_id\"] = agent_task\n",
    "\n",
    "        return self.observation(obs), rew, done, trunc, info\n",
    "\n",
    "\n",
    "class SelfPlay(Curriculum):\n",
    "    def __init__(self, agent, device: str, store_agents_on_cpu: bool = False):\n",
    "        self.store_agents_on_cpu = store_agents_on_cpu\n",
    "        self.storage_device = \"cpu\" if self.store_agents_on_cpu else device\n",
    "        self.agent = deepcopy(agent).to(self.storage_device)\n",
    "        self.task_space = TaskSpace(\n",
    "            spaces.Discrete(1)\n",
    "        )  # SelfPlay can only return agent_id = 0\n",
    "\n",
    "    def update_agent(self, agent: Agent) -> Agent:\n",
    "        self.agent = deepcopy(agent).to(self.storage_device)\n",
    "\n",
    "    def get_opponent(self, agent_id) -> Agent:\n",
    "        if agent_id == None:\n",
    "            agent_id = 0\n",
    "        assert (\n",
    "            agent_id == 0\n",
    "        ), f\"Self play only tracks the current agent. Expected agent id 0, got {agent_id}\"\n",
    "        return self.agent\n",
    "\n",
    "    def sample(self, k=1):\n",
    "        return 0\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            self._layer_init(nn.Linear(3 * 5 * 5, 512)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.actor = self._layer_init(nn.Linear(512, num_actions), std=0.01)\n",
    "        self.critic = self._layer_init(nn.Linear(512, 1))\n",
    "\n",
    "    def _layer_init(self, layer, std=np.sqrt(2), bias_const=0.0):\n",
    "        torch.nn.init.orthogonal_(layer.weight, std)\n",
    "        torch.nn.init.constant_(layer.bias, bias_const)\n",
    "        return layer\n",
    "\n",
    "    def get_value(self, x, flatten_start_dim=1):\n",
    "        x = torch.flatten(x, start_dim=flatten_start_dim)\n",
    "        return self.critic(self.network(x / 255.0))\n",
    "\n",
    "    def get_action_and_value(self, x, action=None, flatten_start_dim=1):\n",
    "        x = torch.flatten(x, start_dim=flatten_start_dim)\n",
    "        hidden = self.network(x / 255.0)\n",
    "        logits = self.actor(hidden)\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(hidden)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    \"\"\"ALGO PARAMS\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    ent_coef = 0.0\n",
    "    vf_coef = 0.5\n",
    "    clip_coef = 0.2\n",
    "    learning_rate = 1e-4\n",
    "    epsilon = 1e-5\n",
    "    gamma = 0.995\n",
    "    gae_lambda = 0.95\n",
    "    epochs = 5\n",
    "    batch_size = 32\n",
    "    stack_size = 3\n",
    "    frame_size = (5, 5)\n",
    "    max_cycles = 201  # lasertag has 200 maximum steps by default\n",
    "    total_episodes = 20\n",
    "    n_agents = 2\n",
    "    num_actions = 5\n",
    "\n",
    "    \"\"\" LEARNER SETUP \"\"\"\n",
    "    agent = Agent(num_actions=num_actions).to(device)\n",
    "    optimizer = optim.Adam(agent.parameters(), lr=learning_rate, eps=epsilon)\n",
    "\n",
    "    \"\"\" ENV SETUP \"\"\"\n",
    "    env = LasertagAdversarial(record_video=False)  # 2 agents by default\n",
    "    env = LasertagParallelWrapper(env=env, n_agents=n_agents)\n",
    "    curriculum = SelfPlay(agent=agent, device=device, store_agents_on_cpu=True)\n",
    "    observation_size = env.observation_space[\"image\"].shape[1:]\n",
    "\n",
    "    \"\"\" ALGO LOGIC: EPISODE STORAGE\"\"\"\n",
    "    end_step = 0\n",
    "    total_episodic_return = 0\n",
    "    rb_obs = torch.zeros((max_cycles, n_agents, stack_size, *frame_size)).to(device)\n",
    "    rb_actions = torch.zeros((max_cycles, n_agents)).to(device)\n",
    "    rb_logprobs = torch.zeros((max_cycles, n_agents)).to(device)\n",
    "    rb_rewards = torch.zeros((max_cycles, n_agents)).to(device)\n",
    "    rb_terms = torch.zeros((max_cycles, n_agents)).to(device)\n",
    "    rb_values = torch.zeros((max_cycles, n_agents)).to(device)\n",
    "\n",
    "    losses, episode_rewards = [], []\n",
    "    agent_c_rew, opp_c_rew = 0, 0\n",
    "    info = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualCurriculumWrapper:\n",
    "    \"\"\"Curriculum wrapper containing both an agent and environment-based curriculum.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: TaskWrapper,\n",
    "        env_curriculum: Curriculum,\n",
    "        agent_curriculum: Curriculum,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.env = env\n",
    "        self.agent_curriculum = agent_curriculum\n",
    "        self.env_curriculum = env_curriculum\n",
    "\n",
    "        self.env_mp_curriculum, self.env_task_queue, self.env_update_queue = (\n",
    "            make_multiprocessing_curriculum(env_curriculum)\n",
    "        )\n",
    "        self.agent_mp_curriculum, self.agent_task_queue, self.agent_update_queue = (\n",
    "            make_multiprocessing_curriculum(agent_curriculum)\n",
    "        )\n",
    "        self.sample()  # initializes env_task and agent_task\n",
    "\n",
    "    def sample(self) -> Tuple[EnvTask, AgentTask]:\n",
    "        \"\"\"Sets new tasks for the environment and agent curricula.\"\"\"\n",
    "        self.env_task = self.env_mp_curriculum.sample()\n",
    "        self.agent_task = self.agent_mp_curriculum.sample()\n",
    "        return self.env_task, self.agent_task\n",
    "\n",
    "    def get_opponent(self, agent_task: AgentTask) -> Tuple[Agent, AgentTask]:\n",
    "        return self.agent_mp_curriculum.curriculum.get_opponent(agent_task)\n",
    "\n",
    "    def update_agent(self, agent: Agent) -> Agent:\n",
    "        return self.agent_mp_curriculum.curriculum.update_agent(agent)\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        \"\"\"Delegate attribute lookup to the curricula if not found.\"\"\"\n",
    "        if hasattr(self.env_curriculum, name):\n",
    "            return getattr(self.env_curriculum, name)\n",
    "        elif hasattr(self.agent_curriculum, name):\n",
    "            return getattr(self.agent_curriculum, name)\n",
    "        else:\n",
    "            raise AttributeError(\n",
    "                f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n",
    "            )\n",
    "\n",
    "\n",
    "agent_curriculum = SelfPlay(agent=agent, device=device, store_agents_on_cpu=True)\n",
    "env_curriculum = DomainRandomization(TaskSpace(spaces.Discrete(200)))\n",
    "curriculum = DualCurriculumWrapper(\n",
    "    env=env,\n",
    "    agent_curriculum=agent_curriculum,\n",
    "    env_curriculum=env_curriculum,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "931f7d9220f742ed920e6496e78c7e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\" TRAINING LOGIC \"\"\"\n",
    "\n",
    "# train for n number of episodes\n",
    "for episode in tqdm(range(total_episodes)):\n",
    "    # collect an episode\n",
    "    with torch.no_grad():\n",
    "        # collect observations and convert to batch of torch tensors\n",
    "        env_task, agent_task = curriculum.sample()\n",
    "        next_obs = env.reset(env_task)\n",
    "        # reset the episodic return\n",
    "        total_episodic_return = 0\n",
    "        n_steps = 0\n",
    "\n",
    "        # each episode has num_steps\n",
    "        for step in range(0, max_cycles):\n",
    "            # rollover the observation\n",
    "            joint_obs = batchify(next_obs, device).squeeze()\n",
    "            agent_obs, opponent_obs = joint_obs\n",
    "\n",
    "            # get action from the agent and the opponent\n",
    "            actions, logprobs, _, values = agent.get_action_and_value(\n",
    "                agent_obs, flatten_start_dim=0\n",
    "            )\n",
    "\n",
    "            opponent = curriculum.get_opponent(info.get(\"agent_id\")).to(device)\n",
    "            opponent_action, *_ = opponent.get_action_and_value(\n",
    "                opponent_obs, flatten_start_dim=0\n",
    "            )\n",
    "            # execute the environment and log data\n",
    "            joint_actions = torch.tensor((actions, opponent_action))\n",
    "            next_obs, rewards, terms, truncs, info = env.step(\n",
    "                unbatchify(joint_actions, env.possible_agents), device, agent_task\n",
    "            )\n",
    "\n",
    "            # add to episode storage\n",
    "            rb_obs[step] = batchify(next_obs, device)\n",
    "            rb_rewards[step] = batchify(rewards, device)\n",
    "            rb_terms[step] = batchify(terms, device)\n",
    "            rb_actions[step] = joint_actions\n",
    "            rb_logprobs[step] = logprobs\n",
    "            rb_values[step] = values.flatten()\n",
    "\n",
    "            # compute episodic return\n",
    "            total_episodic_return += rb_rewards[step].cpu().numpy()\n",
    "\n",
    "            # if we reach termination or truncation, end\n",
    "            if any([terms[a] for a in terms]) or any([truncs[a] for a in truncs]):\n",
    "                end_step = step\n",
    "                break\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_value = agent.get_value(\n",
    "            torch.tensor(next_obs[\"agent_0\"]), flatten_start_dim=0\n",
    "        )\n",
    "        rb_advantages = torch.zeros_like(rb_rewards).to(device)\n",
    "        last_gae_lam = 0\n",
    "        for t in reversed(range(end_step)):\n",
    "            if t == end_step - 1:\n",
    "                next_non_terminal = 1.0 - rb_terms[t + 1]\n",
    "                next_values = next_value\n",
    "            else:\n",
    "                next_non_terminal = 1.0 - rb_terms[t + 1]\n",
    "                next_values = rb_values[t + 1]\n",
    "            delta = (\n",
    "                rb_rewards[t] + gamma * next_values * next_non_terminal - rb_values[t]\n",
    "            )\n",
    "            rb_advantages[t] = last_gae_lam = (\n",
    "                delta + gamma * gae_lambda * next_non_terminal * last_gae_lam\n",
    "            )\n",
    "        rb_returns = rb_advantages + rb_values\n",
    "    # convert our episodes to batch of individual transitions\n",
    "    b_obs = torch.flatten(rb_obs[:end_step], start_dim=0, end_dim=1)\n",
    "    b_logprobs = torch.flatten(rb_logprobs[:end_step], start_dim=0, end_dim=1)\n",
    "    b_actions = torch.flatten(rb_actions[:end_step], start_dim=0, end_dim=1)\n",
    "    b_returns = torch.flatten(rb_returns[:end_step], start_dim=0, end_dim=1)\n",
    "    b_values = torch.flatten(rb_values[:end_step], start_dim=0, end_dim=1)\n",
    "    b_advantages = torch.flatten(rb_advantages[:end_step], start_dim=0, end_dim=1)\n",
    "\n",
    "    # Optimizing the policy and value network\n",
    "    b_index = np.arange(len(b_obs))\n",
    "    clip_fracs = []\n",
    "    for repeat in range(epochs):\n",
    "        # shuffle the indices we use to access the data\n",
    "        np.random.shuffle(b_index)\n",
    "        for start in range(0, len(b_obs), batch_size):\n",
    "            # select the indices we want to train on\n",
    "            end = start + batch_size\n",
    "            batch_index = b_index[start:end]\n",
    "\n",
    "            _, newlogprob, entropy, value = agent.get_action_and_value(\n",
    "                b_obs[batch_index], b_actions.long()[batch_index]\n",
    "            )\n",
    "            logratio = newlogprob - b_logprobs[batch_index]\n",
    "            ratio = logratio.exp()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                old_approx_kl = (-logratio).mean()\n",
    "                approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                clip_fracs += [((ratio - 1.0).abs() > clip_coef).float().mean().item()]\n",
    "\n",
    "            # normalize advantages\n",
    "            rb_advantages = b_advantages[batch_index]\n",
    "            rb_advantages = (rb_advantages - rb_advantages.mean()) / (\n",
    "                rb_advantages.std() + 1e-8\n",
    "            )\n",
    "\n",
    "            # Policy loss\n",
    "            pg_loss1 = -b_advantages[batch_index] * ratio\n",
    "            pg_loss2 = -b_advantages[batch_index] * torch.clamp(\n",
    "                ratio, 1 - clip_coef, 1 + clip_coef\n",
    "            )\n",
    "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "            # Value loss\n",
    "            value = value.flatten()\n",
    "            v_loss_unclipped = (value - b_returns[batch_index]) ** 2\n",
    "            v_clipped = b_values[batch_index] + torch.clamp(\n",
    "                value - b_values[batch_index],\n",
    "                -clip_coef,\n",
    "                clip_coef,\n",
    "            )\n",
    "            v_loss_clipped = (v_clipped - b_returns[batch_index]) ** 2\n",
    "            v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "            v_loss = 0.5 * v_loss_max.mean()\n",
    "\n",
    "            entropy_loss = entropy.mean()\n",
    "            loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
    "            losses.append(loss)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "    var_y = np.var(y_true)\n",
    "    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "    # update opponent\n",
    "    curriculum.update_agent(agent)\n",
    "\n",
    "    agent_c_rew += rewards[\"agent_0\"]\n",
    "    opp_c_rew += rewards[\"agent_1\"]\n",
    "    grid_size = env.level[3][\"grid_size_selected\"]\n",
    "    walls_percentage = env.level[3][\"clutter_rate_selected\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
